---
title: "Code Summary for CAD Project"
author: " FIT3164 Team 07: Danzel, Xinhao and Yiqiu"
date: "2020/8/15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This document contains all codes with comments written for **FIT3164 Team 07 Heart Disease Project** in 2020.  
A useful reference of using R Markdown: <http://rmarkdown.rstudio.com>.

## Section 0: Install and load necessary libraries.

Package Installation:
```{r eval=FALSE}
# install.packages("")
# install.packages("ROCR")
# install.packages("kknn")
# install.packages("randomForest")
# install.packages("e1071")
```

Package Loading:
```{r}
# library()
library(ROCR) # This library is used to calculate AUC.
library(kknn) # This library is used to help training the K-nearest Neighbors model.
library(randomForest) # This library is used to help training the Random Forest model.
library(e1071) # This library is used to help training the Support Vector Machine model.
```

## Section 1: Data Preparation and Understanding.

This section includes reading, understanding and pre-processing of the Z-Alizadeh Sani dataset.  

Note that Z-Alizadeh Sani dataset is available from: <https://archive.ics.uci.edu/ml/machine-learning-databases/00412/>.  

Clear the work space before run the code.
```{r}
rm(list = ls())
```

Read the csv-formatted dataset as a dataframe.
```{r}
ZAS_Original = read.csv("Z_Alizadeh_Sani_Dataset.csv")
ZAS = ZAS_Original
```

Show the first 5 lines of the dataset to see what it looks like.
```{r}
head(ZAS)
```

Check the number of rows (patients) and columns (features).
```{r}
dim(ZAS)
```

There is a useless feature Exertional CP, remove this feature from the dataset (the reason of saying Exertional CP is useless is that it has only one unique value "N" which doesn't help predicting CAD at all).
```{r}
ZAS = ZAS[, !(colnames(ZAS) == "Exertional.CP")]
```

Convert all "factor" variables into "character" type. This makes it easier to convert them into numeric values.
```{r}
for (feature_index in 1:ncol(ZAS)) {
  # Get column name
  feature_name = colnames(ZAS)[feature_index]
   #Convert object to "character" if it is "factor"
  if ((class(ZAS[,feature_name]) == "factor")) {
    ZAS[,feature_name] = as.character(ZAS[,feature_name])
  }
}
```

Convert all categorical features with only "N" / "Y" values to numerical binary (that is, to convert "N" to 0 and "Y" to 1).
```{r}
for (feature_index in 1:ncol(ZAS)) {
  # Get the name of the feature.
  feature_name = colnames(ZAS)[feature_index]
  # For each feature.
  feature = ZAS[, feature_index]
  # Extract unique values of each feature (in vectors).
  unique_val = unlist(c(unique(feature)))
  # If the feature has only "N" and "Y" categorical values, convert "N" to
  # 0 and "Y" to 1:
  if ((length(unique_val) == 2) &
      ("N" %in% unique_val) &
      ("Y" %in% unique_val)) {
    ZAS[ZAS[, feature_name] == "N", feature_name] = 0
    ZAS[ZAS[, feature_name] == "Y", feature_name] = 1
  }
}
```

Convert values of Sex ("Male" / "Fmale") and Cath ("Normal" / "Cad") features to binary (that is to: for Sex, convert "Fmale" to 0 and "Male" to 1; for Cath, convert "Normal" to 1 and "Cad" to 0).
```{r}
# Convert Sex to binary. Male is 1 and Female is 0.
ZAS$Sex[ZAS$Sex == "Male"] = 1
ZAS$Sex[ZAS$Sex == "Fmale"] = 0
# Convert Cath to binary. Normal is 0 and Cad is 1.
ZAS$Cath[ZAS$Cath == "Normal"] = 0
ZAS$Cath[ZAS$Cath == "Cad"] = 1
```

Convert values of features  (Function Class, BBB, Region RWMA and VHD) with more than two categories to dummy.
```{r}
# Function Class.
ZAS$Function.Class = as.factor(ZAS$Function.Class)
contrasts(ZAS$Function.Class) = contr.treatment(4)

# Region RWMA.
ZAS$Region.RWMA = as.factor(ZAS$Region.RWMA)
contrasts(ZAS$Region.RWMA) = contr.treatment(5)

# BBB and VHD.
ZAS$BBB = as.factor(ZAS$BBB)
contrasts(ZAS$BBB) = contr.treatment(3)

# VHD.
ZAS$VHD = as.factor(ZAS$VHD)
contrasts(ZAS$VHD) = contr.treatment(4)

```

All categorical variables are in characters or converted to numeric, this is just for convenience of pre-processing. Now we want them to be factors.
```{r}
for (feature_index in 1:ncol(ZAS)) {
  # Get the name of the feature.
  feature_name = colnames(ZAS)[feature_index]
  # For each feature.
  feature = ZAS[, feature_index]
  # Extract unique values of each feature (in vectors).
  unique_val = unlist(c(unique(feature)))
  # If the feature has only 0 and 1 numerical values:
  if ((length(unique_val) == 2) &
      (0 %in% unique_val) &
      (1 %in% unique_val)) {
    # Convert the feature from "character" to "numeric" object type.
    ZAS[,feature_name] = as.factor(ZAS[,feature_name])
  }
}
```

Check the current type of each feature to ensure they are in expected: there should be 34 categorical features (which should be all factor typed) and 21 quantitative features (which should be either numeric or integer typed).
```{r}
types = sapply(ZAS, class)
print(paste0("The number of factor typed features: ",
             length(grep("factor", types))))
print(paste0("The number of numeric / integer typed features: ", 
             length(grep("numeric", types)) + length(grep("integer", types))))
print(types)
```

Split the dataset into train and test sets (in 70%-30% ratio).
```{r}
# Set the seed to make all teammates have the same random dataset.
set.seed(3164)
# Get 70% of the dataset to be training dataset.
training.rows = sample(1:nrow(ZAS), 0.7*nrow(ZAS))
ZAS.train1 = ZAS[training.rows,]
# The rest of rows are testing dataset.
ZAS.test1 = ZAS[-training.rows,]

# Set the seed to make all teammates have the same random dataset.
set.seed(070707)
# Get 70% of the dataset to be training dataset.
training.rows = sample(1:nrow(ZAS), 0.7*nrow(ZAS))
ZAS.train2 = ZAS[training.rows,]
# The rest of rows are testing dataset.
ZAS.test2 = ZAS[-training.rows,]

# Set the seed to make all teammates have the same random dataset.
set.seed(31643164)
# Get 70% of the dataset to be training dataset.
training.rows = sample(1:nrow(ZAS), 0.7*nrow(ZAS))
ZAS.train3 = ZAS[training.rows,]
# The rest of rows are testing dataset.
ZAS.test3 = ZAS[-training.rows,]

# Set the seed to make all teammates have the same random dataset.
set.seed(13653)
# Get 70% of the dataset to be training dataset.
training.rows = sample(1:nrow(ZAS), 0.7*nrow(ZAS))
ZAS.train4 = ZAS[training.rows,]
# The rest of rows are testing dataset.
ZAS.test4 = ZAS[-training.rows,]

# Set the seed to make all teammates have the same random dataset.
set.seed(1654221)
# Get 70% of the dataset to be training dataset.
training.rows = sample(1:nrow(ZAS), 0.7*nrow(ZAS))
ZAS.train5 = ZAS[training.rows,]
# The rest of rows are testing dataset.
ZAS.test5 = ZAS[-training.rows,]
```

## Section 2: Preliminary model training and evaluation.

In this section, 7 preliminary machine learning classification models will be built and evaluated (Note that preliminary here means default settings are used when training each model).  

Each model will be trained 5 times using 5 train-test sets combination generated in Section 1. The average accuracy & AUC of each model will be calculated, recorded and compared.  

K-nearest neighbor model (KNN):
``` {r}
# Create vectors containing AUC and accuracy of each round's model.
KNN_AUCs = c()
KNN_accuracies = c()
for(index in 1:5) {
  # Iterate through each train-test sets combination.
  train = get(paste0("ZAS.train", index))
  test = get(paste0("ZAS.test", index))
  # Train the model and get the fitted values.
  KNN = kknn(Cath~., train, test)
  fitted_values = fitted(KNN)
  # Calculate and store the AUC.
  pred = prediction(as.numeric(fitted_values), test$Cath)
  AUC = performance(pred, 'auc')@y.values
  KNN_AUCs = append(KNN_AUCs, as.numeric(AUC))
  # Calculate and store the accuracy.
  conf_matrix = table(test$Cath, fitted_values, dnn = c("Actual", "Predicted"))
  accuracy = (conf_matrix[1] + conf_matrix[4]) / sum(conf_matrix)
  KNN_accuracies = append(KNN_accuracies, accuracy)
}
# Calculate and print the average AUC and accuracy of 5-round KNN models.
print(paste0("Average AUC for KNN: ", mean(KNN_AUCs)))
print(paste0("Average accuracy of KNN: ", mean(KNN_accuracies)))
```

Random forest model (RF):
``` {r}
# Create vectors containing AUC and accuracy of each round's model.
RF_AUCs = c()
RF_accuracies = c()
for(index in 1:5) {
  # Iterate through each train-test sets combination.
  train = get(paste0("ZAS.train", index))
  test = get(paste0("ZAS.test", index))
  # Train the model and get the fitted values.
  RF = randomForest(Cath~., data = train)
  test_no_cad = test[, !(colnames(test) %in% c("Cad"))]
  fitted_values = predict(RF, test_no_cad)
  # Calculate and store the AUC.
  pred = prediction(as.numeric(fitted_values), test$Cath)
  AUC = performance(pred, 'auc')@y.values
  RF_AUCs = append(RF_AUCs, as.numeric(AUC))
  # Calculate and store the accuracy.
  conf_matrix = table(test$Cath, fitted_values, dnn = c("Actual", "Predicted"))
  accuracy = (conf_matrix[1] + conf_matrix[4]) / sum(conf_matrix)
  RF_accuracies = append(RF_accuracies, accuracy)
}
# Calculate and print the average AUC and accuracy of 5-round KNN models.
print(paste0("Average AUC for RF: ", mean(RF_AUCs)))
print(paste0("Average accuracy of RF: ", mean(RF_accuracies)))
```

Support Vector Machine (SVM):
``` {r}
# Create vectors containing AUC and accuracy of each round's model.
SVM_AUCs = c()
SVM_accuracies = c()
for(index in 1:5) {
  # Iterate through each train-test sets combination.
  train = get(paste0("ZAS.train", index))
  test = get(paste0("ZAS.test", index))
  # Train the model and get the fitted values.
  SVM = svm(Cath~., data = train)
  test_no_cad = test[, !(colnames(test) %in% c("Cad"))]
  fitted_values = predict(SVM, test_no_cad)
  # Calculate and store the AUC of each round's model.
  pred = prediction(as.numeric(fitted_values), test$Cath)
  AUC = performance(pred, 'auc')@y.values
  SVM_AUCs = append(SVM_AUCs, as.numeric(AUC))
  # Calculate and store the accuracy of each round's model.
  conf_matrix = table(test$Cath, fitted_values, dnn = c("Actual", "Predicted"))
  accuracy = (conf_matrix[1] + conf_matrix[4]) / sum(conf_matrix)
  SVM_accuracies = append(SVM_accuracies, accuracy)
}
# Calculate and print the average AUC and accuracy of 5-round KNN models.
print(paste0("Average AUC for SVM: ", mean(SVM_AUCs)))
print(paste0("Average accuracy of SVM: ", mean(SVM_accuracies)))
```

Naive Bayes:
``` {r}
bayes = naiveBayes(Cath~., data=ZAS.train1)

m.predict = predict(bayes, ZAS.test1)
t2 = table(actual = ZAS.test1$Cath, predicted = m.predict)
#accuracy
Accuracy.Bayes = (t2[1,1]+t2[2,2])/nrow(ZAS.test1)
cat("\n• Naïve Bayes\n")
t2
cat("Accuracy of Naïve Bayes: ", Accuracy.Bayes, "\n")
#Naive Bayes is a bad model that considers almost all the case to be 'no cath'.
```

Decision Tree:
``` {r}
# Create vectors containing AUC and accuracy of each round's model.
Tree_AUCs = c()
Tree_accuracies = c()
for(index in 1:5) {
  # Iterate through each train-test sets combination.
  train = get(paste0("ZAS.train", index))
  test = get(paste0("ZAS.test", index))
  # Train the model and get the fitted values.
  r.fit=tree(Cath~., data=train)
  m.predict = predict(r.fit, test, type = "class")
  t1 = table(actual = test$Cath, predicted = m.predict)
  # Accuracy
  Accuracy.Tree = (t1[1,1]+t1[2,2])/nrow(test)
  Tree_accuracies = append(Tree_accuracies, Accuracy.Tree)
  
  # do predictions as probabilities and draw ROC
  pred.tree = predict(r.fit, test, type = "vector")
  # computing a simple ROC curve (x-axis: fpr, y-axis: tpr)
  # labels are actual values, predictors are probability of class 
  Rpred <- ROCR::prediction(pred.tree[,2], test[["Cath"]]) 
  cauc = performance(Rpred, "auc")
  Tree_AUCs = append(Tree_AUCs, as.numeric(cauc@y.values))

}
cat(Tree_AUCs)
cat(Tree_accuracies)
# Average
print(paste0("Average AUC for decsion tree: ", round(mean(Tree_AUCs),digits = 5)))
print(paste0("Average accuracy of decision tree: ", round(mean(Tree_accuracies),digits = 5)))
```
