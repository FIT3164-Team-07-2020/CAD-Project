---
title: "Code Summary for CAD Project"
author: " FIT3164 Team 07: Danzel, Xinhao and Yiqiu"
date: "2020/8/15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This document contains all codes with comments written for **FIT3164 Team 07 Heart Disease Project** in 2020.  
A useful reference of using R Markdown: <http://rmarkdown.rstudio.com>.

## Section 0: Install and load necessary libraries.

Package Installation:
```{r eval=FALSE}
# install.packages("")
# install.packages("caret")
# install.packages("ggplot2")
# install.packages("ROCR")
# install.packages("kknn")
# install.packages("randomForest")
# install.packages("e1071")
# install.packages("adabag")
```

Package Loading:
```{r, warning = FALSE}
# library()
library(caret) # This library is used to pre-process the data.
library(ggplot2) # This library is used to plot any necessary chart / graph.
library(ROCR) # This library is used to calculate AUC.
library(kknn) # This library is used to help training the K-nearest Neighbors model.
library(randomForest) # This library is used to help training the Random Forest model.
library(e1071) # This library is used to help training the Support Vector Machine model.
library(tree) #This library is used to build decision tree.
library(adabag) # This library is used to build adaptive boosting and bagging models
library(mltools) #Primarily used for one hot encoding.
library(data.table) #Used for converting data frame to a data.table, which makes it  easier to perform one hot encoding.
```

## Section 1: Data Preparation and Understanding.

This section includes reading, understanding and pre-processing of the Z-Alizadeh Sani dataset.  

Note that Z-Alizadeh Sani dataset is available from: <https://archive.ics.uci.edu/ml/machine-learning-databases/00412/>.  

Clear the work space before run the code.
```{r}
rm(list = ls())
```

Read the csv-formatted dataset as a dataframe.
```{r}
ZAS_Original = read.csv("Z_Alizadeh_Sani_Dataset.csv")
ZAS = ZAS_Original
```

Show the first 5 lines of the dataset to see what it looks like.
```{r}
head(ZAS)
```

Check the number of rows (patients) and columns (features).
```{r}
dim(ZAS)
```

Remove predictor features with zero / nearly zero variances (the reason is that the features have extremely unbalanced value distribution which don't significantly help predicting CAD).
```{r}
# Find predictor features with zero / nearly-zero variances.
# Record their column indexes.
nzv_feature_indexes = nearZeroVar(ZAS[, !(colnames(ZAS) %in% c("Cath"))])
# Print selected features information.
print(paste0("Number of predictor features with zero or near-zero variances: ",
             length(nzv_feature_indexes)))
print(paste0("Following predictor features have zero or near-zero variances and should be removed: ",
             toString(colnames(ZAS)[nzv_feature_indexes])))
# Remove predictor features with zero / nearly-zero variances.
ZAS = ZAS[, -nzv_feature_indexes]
```

Convert all "factor" variables into "character" type. This makes it easier to convert them into numeric values.
```{r}
for (feature_index in 1:ncol(ZAS)) {
  # Get column name
  feature_name = colnames(ZAS)[feature_index]
   #Convert object to "character" if it is "factor"
  if ((class(ZAS[,feature_name]) == "factor")) {
    ZAS[,feature_name] = as.character(ZAS[,feature_name])
  }
}
```

Convert all categorical features with only "N" / "Y" values to numerical binary (that is, to convert "N" to 0 and "Y" to 1).
```{r}
for (feature_index in 1:ncol(ZAS)) {
  # Get the name of the feature.
  feature_name = colnames(ZAS)[feature_index]
  # For each feature.
  feature = ZAS[, feature_index]
  # Extract unique values of each feature (in vectors).
  unique_val = unlist(c(unique(feature)))
  # If the feature has only "N" and "Y" categorical values, convert "N" to
  # 0 and "Y" to 1:
  if ((length(unique_val) == 2) &
      ("N" %in% unique_val) &
      ("Y" %in% unique_val)) {
    ZAS[ZAS[, feature_name] == "N", feature_name] = 0
    ZAS[ZAS[, feature_name] == "Y", feature_name] = 1
  }
}
```

Convert values of Sex ("Male" / "Fmale") and Cath ("Normal" / "Cad") features to binary (that is to: for Sex, convert "Fmale" to 0 and "Male" to 1; for Cath, convert "Normal" to 1 and "Cad" to 0).
```{r}
# Convert Sex to binary. Male is 1 and Female is 0.
ZAS$Sex[ZAS$Sex == "Male"] = 1
ZAS$Sex[ZAS$Sex == "Fmale"] = 0
# Convert Cath to binary. Normal is 0 and Cad is 1.
ZAS$Cath[ZAS$Cath == "Normal"] = 0
ZAS$Cath[ZAS$Cath == "Cad"] = 1
```

Convert values of features (Function Class, Region RWMA and VHD) with more than two categories to dummy variables via one hot encoding.
```{r}
#Convert all to factor first so the variables are the same class.
ZAS$Function.Class = as.factor(ZAS$Function.Class)
ZAS$Region.RWMA = as.factor(ZAS$Region.RWMA)
ZAS$VHD = as.factor(ZAS$VHD)

#Converts ZAS to a data.table, which is just a convenient format for performing one hot encoding with the mltools package.
ZAS = data.table(ZAS)

#One hot encoding using mltools package.
ZAS = one_hot(ZAS, cols = c('Function.Class', 'Region.RWMA', 'VHD'))

#Convert back to data frame
ZAS = data.frame(ZAS)

```

All categorical variables are in characters or converted to numeric, this is just for convenience of pre-processing. Now we want them to be factors.
```{r}
for (feature_index in 1:ncol(ZAS)) {
  # Get the name of the feature.
  feature_name = colnames(ZAS)[feature_index]
  # For each feature.
  feature = ZAS[, feature_index]
  # Extract unique values of each feature (in vectors).
  unique_val = unlist(c(unique(feature)))
  # If the feature has only 0 and 1 numerical values:
  if ((length(unique_val) == 2) &
      (0 %in% unique_val) &
      (1 %in% unique_val)) {
    # Convert the feature from "character" to "numeric" object type.
    ZAS[,feature_name] = as.factor(ZAS[,feature_name])
  }
}
```

Check the current type of each feature to ensure they are in expected: there should be 30 categorical features (which should be all factor typed) and 21 quantitative features (which should be either numeric or integer typed).
```{r}
types = sapply(ZAS, class)
print(paste0("The number of factor typed features: ",
             length(grep("factor", types))))
print(paste0("The number of numeric / integer typed features: ", 
             length(grep("numeric", types)) + length(grep("integer", types))))
print(types)
```

Split the dataset into train and test sets (in 70%-30% ratio).
```{r}
# Set the seed to make all teammates have the same random dataset.
set.seed(3164)
# Get 70% of the dataset to be training dataset.
training.rows = sample(1:nrow(ZAS), 0.7*nrow(ZAS))
ZAS.train1 = ZAS[training.rows,]
# The rest of rows are testing dataset.
ZAS.test1 = ZAS[-training.rows,]

# Set the seed to make all teammates have the same random dataset.
set.seed(070707)
# Get 70% of the dataset to be training dataset.
training.rows = sample(1:nrow(ZAS), 0.7*nrow(ZAS))
ZAS.train2 = ZAS[training.rows,]
# The rest of rows are testing dataset.
ZAS.test2 = ZAS[-training.rows,]

# Set the seed to make all teammates have the same random dataset.
set.seed(31643164)
# Get 70% of the dataset to be training dataset.
training.rows = sample(1:nrow(ZAS), 0.7*nrow(ZAS))
ZAS.train3 = ZAS[training.rows,]
# The rest of rows are testing dataset.
ZAS.test3 = ZAS[-training.rows,]

# Set the seed to make all teammates have the same random dataset.
set.seed(13653)
# Get 70% of the dataset to be training dataset.
training.rows = sample(1:nrow(ZAS), 0.7*nrow(ZAS))
ZAS.train4 = ZAS[training.rows,]
# The rest of rows are testing dataset.
ZAS.test4 = ZAS[-training.rows,]

# Set the seed to make all teammates have the same random dataset.
set.seed(1654221)
# Get 70% of the dataset to be training dataset.
training.rows = sample(1:nrow(ZAS), 0.7*nrow(ZAS))
ZAS.train5 = ZAS[training.rows,]
# The rest of rows are testing dataset.
ZAS.test5 = ZAS[-training.rows,]
```

## Section 2: Preliminary model training and evaluation.

In this section, 7 preliminary machine learning classification models will be built and evaluated (Note that preliminary here means default settings are used when training each model).  

Each model will be trained 5 times using 5 train-test sets combination generated in Section 1. The average accuracy & AUC of each model will be calculated, recorded and compared.  

K-nearest neighbor:
``` {r}
# Create vectors containing AUC and accuracy of each round's model.
KNN_AUCs = c()
KNN_accuracies = c()
for(index in 1:5) {
  # Iterate through each train-test sets combination.
  train = get(paste0("ZAS.train", index))
  test = get(paste0("ZAS.test", index))
  # Train the model and get the fitted values.
  KNN = kknn(Cath~., train, test)
  fitted_values = fitted(KNN)
  # Calculate and store the AUC.
  pred = prediction(as.numeric(fitted_values), test$Cath)
  AUC = performance(pred, 'auc')@y.values
  KNN_AUCs = append(KNN_AUCs, as.numeric(AUC))
  # Calculate and store the accuracy.
  conf_matrix = table(test$Cath, fitted_values, dnn = c("Actual", "Predicted"))
  accuracy = (conf_matrix[1] + conf_matrix[4]) / sum(conf_matrix)
  KNN_accuracies = append(KNN_accuracies, accuracy)
}
# Calculate and print the average AUC and accuracy of 5-round KNN models.
print(paste0("Average AUC for KNN: ", mean(KNN_AUCs)))
print(paste0("Average accuracy of KNN: ", mean(KNN_accuracies)))
```

Random forest:
``` {r}
# Create vectors containing AUC and accuracy of each round's model.
RF_AUCs = c()
RF_accuracies = c()
RF_importance = as.data.frame((matrix(ncol = 0, nrow = 50)))
for(index in 1:5) {
  # Iterate through each train-test sets combination.
  train = get(paste0("ZAS.train", index))
  test = get(paste0("ZAS.test", index))
  # Train the model and get the fitted values.
  RF = randomForest(Cath~., data = train)
  test_no_cath = test[, !(colnames(test) %in% c("Cath"))]
  fitted_values = predict(RF, test_no_cath)
  # Calculate and store the AUC.
  pred = prediction(as.numeric(fitted_values), test$Cath)
  AUC = performance(pred, 'auc')@y.values
  RF_AUCs = append(RF_AUCs, as.numeric(AUC))
  # Calculate and store the accuracy.
  conf_matrix = table(test$Cath, fitted_values, dnn = c("Actual", "Predicted"))
  accuracy = (conf_matrix[1] + conf_matrix[4]) / sum(conf_matrix)
  RF_accuracies = append(RF_accuracies, accuracy)
  # Calculate and store the features importance in each round.
  current_RF_imp = as.data.frame(RF$importance)
  RF_importance = cbind(RF_importance, current_RF_imp)
  names(RF_importance)[length(names(RF_importance))] = paste0("Round_", index)
}
# Calculate, store and show each feature's average importance.
RF_importance = as.data.frame(sort(apply(RF_importance, 1, mean),
                                   decreasing = TRUE))
colnames(RF_importance) = "Average Importance"
print(RF_importance)
# Plot the the importance of 10 most influential features in random forest.
RF_ten_best_features = head(RF_importance, 10)
hbc_rf = ggplot(data = RF_ten_best_features, 
       aes(x = rownames(RF_ten_best_features),
           y = RF_ten_best_features[,1])) +
  geom_col()+ xlab("Feature") + ylab("Importance Score") + coord_flip() +
  ggtitle("10 Most Influential Features in Random Forest")
print(hbc_rf)
# Calculate and print the average AUC and accuracy of 5-round KNN models.
print(paste0("Average AUC for RF: ", mean(RF_AUCs)))
print(paste0("Average accuracy of RF: ", mean(RF_accuracies)))
```

Support Vector Machine:
``` {r}
# Create vectors containing AUC and accuracy of each round's model.
SVM_AUCs = c()
SVM_accuracies = c()
for(index in 1:5) {
  # Iterate through each train-test sets combination.
  train = get(paste0("ZAS.train", index))
  test = get(paste0("ZAS.test", index))
  # Train the model and get the fitted values.
  SVM = svm(Cath~., data = train)
  test_no_cath = test[, !(colnames(test) %in% c("Cath"))]
  fitted_values = predict(SVM, test_no_cath)
  # Calculate and store the AUC of each round's model.
  pred = prediction(as.numeric(fitted_values), test$Cath)
  AUC = performance(pred, 'auc')@y.values
  SVM_AUCs = append(SVM_AUCs, as.numeric(AUC))
  # Calculate and store the accuracy of each round's model.
  conf_matrix = table(test$Cath, fitted_values, dnn = c("Actual", "Predicted"))
  accuracy = (conf_matrix[1] + conf_matrix[4]) / sum(conf_matrix)
  SVM_accuracies = append(SVM_accuracies, accuracy)
}
# Calculate and print the average AUC and accuracy of 5-round KNN models.
print(paste0("Average AUC for SVM: ", mean(SVM_AUCs)))
print(paste0("Average accuracy of SVM: ", mean(SVM_accuracies)))
```

Naive Bayes:
``` {r}
bayes = naiveBayes(Cath~., data=ZAS.train1)
Bayes_AUCs = c()
Bayes_accuracies = c()
for(index in 1:5) {
  # Iterate through each train-test sets combination.
  train = get(paste0("ZAS.train", index))
  test = get(paste0("ZAS.test", index))
  # Train the model and get the fitted values.
  r.fit=naiveBayes(Cath~., data=train)
  m.predict = predict(r.fit, test)
  t1 = table(actual = test$Cath, predicted = m.predict)
  # Accuracy
  Accuracy.Tree = (t1[1,1]+t1[2,2])/nrow(test)
  Bayes_accuracies = append(Bayes_accuracies, Accuracy.Tree)

  # AUC
  Rpred.bayes = predict(r.fit, test, type = 'raw') 
  Rpred <- ROCR::prediction( Rpred.bayes[,2], test[["Cath"]]) 
  cauc = performance(Rpred, "auc")
  AUC.Bayes = as.numeric(cauc@y.values)
  Bayes_AUCs = append(Bayes_AUCs, as.numeric(cauc@y.values))

}
# Average
cat(paste0("Average AUC for Naive Bayes: ", round(mean(Bayes_AUCs),digits = 5)))
cat("\n")
cat(paste0("Average accuracy of Naive Bayes: ", round(mean(Bayes_accuracies),digits = 5)))
```

Decision Tree:
``` {r}
# Create vectors containing AUC and accuracy of each round's model.
Tree_AUCs = c()
Tree_accuracies = c()
for(index in 1:5) {
  # Iterate through each train-test sets combination.
  train = get(paste0("ZAS.train", index))
  test = get(paste0("ZAS.test", index))
  # Train the model and get the fitted values.
  r.fit=tree(Cath~., data=train)
  m.predict = predict(r.fit, test, type = "class")
  t1 = table(actual = test$Cath, predicted = m.predict)
  # Accuracy
  Accuracy.Tree = (t1[1,1]+t1[2,2])/nrow(test)
  Tree_accuracies = append(Tree_accuracies, Accuracy.Tree)
  
  # do predictions as probabilities and draw ROC
  pred.tree = predict(r.fit, test, type = "vector")
  # computing a simple ROC curve (x-axis: fpr, y-axis: tpr)
  # labels are actual values, predictors are probability of class 
  Rpred <- ROCR::prediction(pred.tree[,2], test[["Cath"]]) 
  cauc = performance(Rpred, "auc")
  Tree_AUCs = append(Tree_AUCs, as.numeric(cauc@y.values))

}
# Average
cat(paste0("Average AUC for decsion tree: ", round(mean(Tree_AUCs),digits = 5)))
cat("\n")
cat(paste0("Average accuracy of decision tree: ", round(mean(Tree_accuracies),digits = 5)))
```

Boosting (Adaptive)

```{r}
#Boosting
# Create vectors containing AUC and accuracy of each round's model.
Boost_AUCs = c()
Boost_accuracies = c()
boost_importance = as.data.frame((matrix(ncol = 0, nrow = 50)))

for(index in 1:5) {
  # Iterate through each train-test sets combination.
  train = get(paste0("ZAS.train", index))
  test = get(paste0("ZAS.test", index))
  # Train the model and get the fitted values.
  boost = boosting(Cath~., data = train)
  test_no_cad = test[, !(colnames(test) %in% c("Cad"))]
  boostpredict = predict.boosting(boost, newdata = test_no_cad)
  #Calculate accuracy of the model
  n = sum(boostpredict$confusion)
  x = diag(boostpredict$confusion)
  accuracy = sum(x)/n
  Boost_accuracies = append(Boost_accuracies, accuracy)
  #Get AUC
  boostprob = prediction(boostpredict$prob[,2], test$Cath)
  AUC = performance(boostprob, "auc")@y.values
  Boost_AUCs = append(Boost_AUCs, as.numeric(AUC))
    # Calculate and store the features importance in each round.
  current_boost_imp = as.data.frame(boost$importance)
  boost_importance = cbind(boost_importance, current_boost_imp)
  names(boost_importance)[length(names(boost_importance))] = paste0("Round_", index)
}
# Calculate, store and show each feature's average importance.
boost_importance = as.data.frame(sort(apply(boost_importance, 1, mean), decreasing = TRUE))
colnames(boost_importance) = "Average Importance"
print(boost_importance)
# Plot the the importance of 10 most influential features in boosting.
boost_ten_best_features = head(boost_importance, 10)
hbc_boost = ggplot(data = boost_ten_best_features, aes(x = rownames(boost_ten_best_features), y = boost_ten_best_features[,1])) +
  geom_col()+ xlab("Feature") + ylab("Importance Score") + coord_flip() +
  ggtitle("10 Most Influential Features in boosting")
print(hbc_boost)
print(paste0("Average AUC for boosting: ", mean(Boost_AUCs)))
print(paste0("Average accuracy of boosting: ", mean(Boost_accuracies)))

```

Boosting (Gradient)

```{r message=FALSE, warning=FALSE}
#Gradient Boosting
# Create vectors containing AUC and accuracy of each round's model.
gradb_AUCs = c()
gradb_accuracies = c()
for(index in 1:5) {
  # Iterate through each train-test sets combination.
  train = get(paste0("ZAS.train", index))
  test = get(paste0("ZAS.test", index))
  # Train the model and get the fitted values.
  gradb = train(Cath~., data = train, method = "gbm", verbose = FALSE)
  test_no_cad = test[, !(colnames(test) %in% c("Cad"))]
  gradbpredict = predict(gradb, test_no_cad)
  #Calculate accuracy
  conf_matrix = table(test$Cath, gradbpredict, dnn = c("Actual", "Predicted"))
  accuracy = (conf_matrix[1] + conf_matrix[4]) / sum(conf_matrix)
  gradb_accuracies = append(gradb_accuracies, accuracy)
  #Get AUC
  gradbprob = prediction(as.numeric(gradbpredict), test$Cath)
  AUC = performance(gradbprob, "auc")@y.values
  gradb_AUCs = append(gradb_AUCs, as.numeric(AUC))
}
print(paste0("Average AUC for Gradient Boosting: ", mean(gradb_AUCs)))
print(paste0("Average accuracy of Gradient Boosting: ", mean(gradb_accuracies)))

```


Bagging

```{r}
#Bagging
# Create vectors containing AUC and accuracy of each round's model.
Bagg_AUCs = c()
Bagg_accuracies = c()
bag_importance = as.data.frame((matrix(ncol = 0, nrow = 50)))
for(index in 1:5) {
  # Iterate through each train-test sets combination.
  train = get(paste0("ZAS.train", index))
  test = get(paste0("ZAS.test", index))
  # Train the model and get the fitted values.
  bagg = bagging(Cath~., data = train)
  test_no_cad = test[, !(colnames(test) %in% c("Cad"))]
  baggpredict = predict.bagging(bagg, newdata = test_no_cad)
  #Calculate accuracy of the model
  n = sum(baggpredict$confusion)
  x = diag(baggpredict$confusion)
  accuracy = sum(x)/n
  Bagg_accuracies = append(Bagg_accuracies, accuracy)
  #Get AUC
  baggprob = prediction(baggpredict$prob[,2], test$Cath)
  AUC = performance(baggprob, "auc")@y.values
  Bagg_AUCs = append(Bagg_AUCs, as.numeric(AUC))
  # Calculate and store the features importance in each round.
  current_bag_imp = as.data.frame(bagg$importance)
  bag_importance = cbind(bag_importance, current_bag_imp)
  names(bag_importance)[length(names(bag_importance))] = paste0("Round_", index)
}
# Calculate, store and show each feature's average importance.
bag_importance = as.data.frame(sort(apply(bag_importance, 1, mean), decreasing = TRUE))
colnames(bag_importance) = "Average Importance"
print(bag_importance)
# Plot the the importance of 10 most influential features in bagging.
bag_ten_best_features = head(bag_importance, 10)
hbc_bag = ggplot(data = bag_ten_best_features, aes(x = rownames(bag_ten_best_features), y = bag_ten_best_features[,1])) +
  geom_col()+ xlab("Feature") + ylab("Importance Score") + coord_flip() +
  ggtitle("10 Most Influential Features in Bagging")
print(hbc_bag)
print(paste0("Average AUC for Bagging: ", mean(Bagg_AUCs)))
print(paste0("Average accuracy of Bagging: ", mean(Bagg_accuracies)))
```


## Section 3: Feature selection from ensemble models.

In this section, 2 compressed datasets will be produced by removing some less important features. This is based on the feature importance information from ensemble models trained in Section 2. Details of feature importance analysis 
will be included in the final report.
``` {r}
# Cut 1 threshold.
cut_1 = c("Q.Wave", "Obesity", "Systolic.Murmur", "LVH", "FH")
ZAS.compressed1 = ZAS[, !(colnames(ZAS) %in% cut_1)]
print(dim(ZAS.compressed1))
# write.csv(ZAS.compressed1, "ZAS.compressed1.csv")

# Cut 2 threshold.
cut_2 = c("Q.Wave", "Obesity", "Systolic.Murmur", "LVH", "FH", "Sex", "DLP", "Function.Class_0", "Function.Class_1", "Function.Class_2", "Function.Class_3", "St.Depression", "DM", "Current.Smoker", "Dyspnea", "VHD_mild", "VHD_Moderate", "VHD_N", "VHD_Severe", "Nonanginal", "HDL", "Neut")
ZAS.compressed2 = ZAS[, !(colnames(ZAS) %in% cut_2)]
print(dim(ZAS.compressed2))
# write.csv(ZAS.compressed2, "ZAS.compressed2.csv")
```
