---
title: "Model Training Summary for CAD Web Page"
author: " FIT3164 Team 07: Denzel, Xinhao and Yiqiu"
date: "2020/8/15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This document contains all codes with comments written for **FIT3164 Team 07 Heart Disease Project** in 2020.  

## Section 0: Install and load necessary libraries.

Package Loading:
```{r, warning = FALSE, message = FALSE}
# library()
library(caret) # This library is used to pre-process the data.
library(neuralnet) # Used for training neural network.
library(ggplot2) # This library is used to plot any necessary chart / graph.
library(ROCR) # This library is used to calculate AUC.
library(kknn) # This library is used to help training the K-nearest Neighbors model.
library(randomForest) # This library is used to help training the Random Forest model.
library(e1071) # This library is used to help training the Support Vector Machine model.
library(tree) # This library is used to build decision tree.
library(adabag) # This library is used to build adaptive boosting and bagging models
library(mltools) # Primarily used for one hot encoding.
library(data.table) # Used for converting data frame to a data.table, which makes it  easier to perform one hot encoding.
```

## Section 1: Data Preparation and Understanding.

This section includes reading, understanding and pre-processing of the Z-Alizadeh Sani dataset.  

Note that Z-Alizadeh Sani dataset is available from: <https://archive.ics.uci.edu/ml/machine-learning-databases/00412/>.  

Clear the work space before run the code.
```{r}
rm(list = ls())
```

Read the csv-formatted dataset as a dataframe.
```{r}
ZAS_Original = read.csv("Z_Alizadeh_Sani_Dataset.csv")
ZAS = ZAS_Original
```

Show the first 5 lines of the dataset to see what it looks like.
```{r}
head(ZAS)
```

Check the number of rows (patients) and columns (features).
```{r}
dim(ZAS)
```

We need to remove predictor features with zero / nearly zero variances as features that have extremely unbalanced value distribution have little to no impact in helping predicting CAD).

First we'll find the features with zero/near-zero variances.
```{r}
# Find predictor features with zero / nearly-zero variances.
# Record their column indexes.
nzv_feature_indexes = nearZeroVar(ZAS[, !(colnames(ZAS) %in% c("Cath"))])
# Print selected features information.
print(paste0("Number of predictor features with zero or near-zero variances: ",
             length(nzv_feature_indexes)))
print(paste0("Following predictor features have zero or near-zero variances and should be removed: ",
             toString(colnames(ZAS)[nzv_feature_indexes])))
```

Now we'll remove all of these variables from our dataset.
```{r}
# Remove predictor features with zero / nearly-zero variances.
ZAS = ZAS[, -nzv_feature_indexes]
```


We now need to convert "factor" variables into "character" type. This makes it easier to convert them into numeric values, which is necessary for later encoding our categorical variables.
```{r}
for (feature_index in 1:ncol(ZAS)) {
  # Get column name
  feature_name = colnames(ZAS)[feature_index]
   #Convert object to "character" if it is "factor"
  if ((class(ZAS[,feature_name]) == "factor")) {
    ZAS[,feature_name] = as.character(ZAS[,feature_name])
  }
}
```

Convert all categorical features with only "N" / "Y" values to numerical binary (that is, to convert "N" to 0 and "Y" to 1).
```{r}
for (feature_index in 1:ncol(ZAS)) {
  # Get the name of the feature.
  feature_name = colnames(ZAS)[feature_index]
  # For each feature.
  feature = ZAS[, feature_index]
  # Extract unique values of each feature (in vectors).
  unique_val = unlist(c(unique(feature)))
  # If the feature has only "N" and "Y" categorical values, convert "N" to
  # 0 and "Y" to 1:
  if ((length(unique_val) == 2) &
      ("N" %in% unique_val) &
      ("Y" %in% unique_val)) {
    ZAS[ZAS[, feature_name] == "N", feature_name] = 0
    ZAS[ZAS[, feature_name] == "Y", feature_name] = 1
  }
}
```

Convert values of Sex ("Male" / "Fmale") and Cath ("Normal" / "Cad") features to binary (that is to: for Sex, convert "Fmale" to 0 and "Male" to 1; for Cath, convert "Normal" to 1 and "Cad" to 0).
```{r}
# Convert Sex to binary. Male is 1 and Female is 0.
ZAS$Sex[ZAS$Sex == "Male"] = 1
ZAS$Sex[ZAS$Sex == "Fmale"] = 0
# Convert Cath to binary. Normal is 0 and Cad is 1.
ZAS$Cath[ZAS$Cath == "Normal"] = 0
ZAS$Cath[ZAS$Cath == "Cad"] = 1
```

Now we need to perform one hot encoding on categorical variables with more than two factor levels. In this case we convert the variables Function Class, Region RWMA and VHD.

```{r}
#Convert all to factor first so the variables are the same class.
ZAS$Function.Class = as.factor(ZAS$Function.Class)
ZAS$Region.RWMA = as.factor(ZAS$Region.RWMA)
ZAS$VHD = as.factor(ZAS$VHD)

#Converts ZAS to a data.table, which is just a convenient format for performing one hot encoding with the mltools package.
ZAS = data.table(ZAS)

#One hot encoding using mltools package.
ZAS = one_hot(ZAS, cols = c('Function.Class', 'Region.RWMA', 'VHD'))

#Convert back to data frame
ZAS = data.frame(ZAS)

```

All categorical variables are in characters or converted to numeric, this is just for convenience of pre-processing. Now we want them to be factors.
```{r}
for (feature_index in 1:ncol(ZAS)) {
  # Get the name of the feature.
  feature_name = colnames(ZAS)[feature_index]
  # For each feature.
  feature = ZAS[, feature_index]
  # Extract unique values of each feature (in vectors).
  unique_val = unlist(c(unique(feature)))
  # If the feature has only 0 and 1 numerical values:
  if ((length(unique_val) == 2) &
      (0 %in% unique_val) &
      (1 %in% unique_val)) {
    # Convert the feature from "character" to "numeric" object type.
    ZAS[,feature_name] = as.factor(ZAS[,feature_name])
  }
}
```

We'll now check the current type of each feature to ensure they are in expected: there should be 30 categorical features (which should be all factor typed) and 21 quantitative features (which should be either numeric or integer typed).
```{r}
types = sapply(ZAS, class)
print(paste0("The number of factor typed features: ",
             length(grep("factor", types))))
print(paste0("The number of numeric / integer typed features: ", 
             length(grep("numeric", types)) + length(grep("integer", types))))
print(types)
```

## Creating Testing and Training datasets

For our intial model training phase, we'll create five different testing/training sets and use them to train our models.

Split the dataset into train and test sets (in 70%-30% ratio).
```{r}
# Set the seed to make all teammates have the same random dataset.
set.seed(3164)
# Get 70% of the dataset to be training dataset.
training.rows = sample(1:nrow(ZAS), 0.7*nrow(ZAS))
ZAS.train1 = ZAS[training.rows,]
# The rest of rows are testing dataset.
ZAS.test1 = ZAS[-training.rows,]

# Set the seed to make all teammates have the same random dataset.
set.seed(070707)
# Get 70% of the dataset to be training dataset.
training.rows = sample(1:nrow(ZAS), 0.7*nrow(ZAS))
ZAS.train2 = ZAS[training.rows,]
# The rest of rows are testing dataset.
ZAS.test2 = ZAS[-training.rows,]

# Set the seed to make all teammates have the same random dataset.
set.seed(31643164)
# Get 70% of the dataset to be training dataset.
training.rows = sample(1:nrow(ZAS), 0.7*nrow(ZAS))
ZAS.train3 = ZAS[training.rows,]
# The rest of rows are testing dataset.
ZAS.test3 = ZAS[-training.rows,]

# Set the seed to make all teammates have the same random dataset.
set.seed(13653)
# Get 70% of the dataset to be training dataset.
training.rows = sample(1:nrow(ZAS), 0.7*nrow(ZAS))
ZAS.train4 = ZAS[training.rows,]
# The rest of rows are testing dataset.
ZAS.test4 = ZAS[-training.rows,]

# Set the seed to make all teammates have the same random dataset.
set.seed(1654221)
# Get 70% of the dataset to be training dataset.
training.rows = sample(1:nrow(ZAS), 0.7*nrow(ZAS))
ZAS.train5 = ZAS[training.rows,]
# The rest of rows are testing dataset.
ZAS.test5 = ZAS[-training.rows,]
```

## Section 2: Preliminary model training and evaluation.

In this section, 7 preliminary machine learning classification models will be built and evaluated (Note that preliminary here means default settings are used when training each model).  

Each model will be trained 5 times using 5 train-test sets combination generated in Section 1. The average accuracy & AUC of each model will be calculated, recorded and compared.  

## K-nearest neighbor:
``` {r}
# Create vectors containing AUC and accuracy of each round's model.
KNN_AUCs = c()
KNN_accuracies = c()
for(index in 1:5) {
  # Iterate through each train-test sets combination.
  train = get(paste0("ZAS.train", index))
  test = get(paste0("ZAS.test", index))
  # Train the model and get the fitted values.
  KNN = kknn(Cath~., train, test)
  fitted_values = fitted(KNN)
  # Calculate and store the AUC.
  pred = prediction(as.numeric(fitted_values), test$Cath)
  AUC = performance(pred, 'auc')@y.values
  KNN_AUCs = append(KNN_AUCs, as.numeric(AUC))
  # Calculate and store the accuracy.
  conf_matrix = table(test$Cath, fitted_values, dnn = c("Actual", "Predicted"))
  accuracy = (conf_matrix[1] + conf_matrix[4]) / sum(conf_matrix)
  KNN_accuracies = append(KNN_accuracies, accuracy)
}
# Calculate and print the average AUC and accuracy of 5-round KNN models.
print(paste0("Average AUC for KNN: ", mean(KNN_AUCs)))
print(paste0("Average accuracy of KNN: ", mean(KNN_accuracies)))
```

## Random forest:
``` {r}
# Create vectors containing AUC and accuracy of each round's model.
RF_AUCs = c()
RF_accuracies = c()
RF_importance = as.data.frame((matrix(ncol = 0, nrow = 50)))
for(index in 1:5) {
  # Iterate through each train-test sets combination.
  train = get(paste0("ZAS.train", index))
  test = get(paste0("ZAS.test", index))
  # Train the model and get the fitted values.
  RF = randomForest(Cath~., data = train)
  test_no_cath = test[, !(colnames(test) %in% c("Cath"))]
  fitted_values = predict(RF, test_no_cath)
  # Calculate and store the AUC.
  pred = prediction(as.numeric(fitted_values), test$Cath)
  AUC = performance(pred, 'auc')@y.values
  RF_AUCs = append(RF_AUCs, as.numeric(AUC))
  # Calculate and store the accuracy.
  conf_matrix = table(test$Cath, fitted_values, dnn = c("Actual", "Predicted"))
  accuracy = (conf_matrix[1] + conf_matrix[4]) / sum(conf_matrix)
  RF_accuracies = append(RF_accuracies, accuracy)
  # Calculate and store the features importance in each round.
  current_RF_imp = as.data.frame(RF$importance)
  RF_importance = cbind(RF_importance, current_RF_imp)
  names(RF_importance)[length(names(RF_importance))] = paste0("Round_", index)
}
# Calculate, store and show each feature's average importance.
RF_importance = as.data.frame(sort(apply(RF_importance, 1, mean),
                                   decreasing = TRUE))
colnames(RF_importance) = "Average Importance"
print(RF_importance)
# Plot the the importance of 10 most influential features in random forest.
RF_ten_best_features = head(RF_importance, 10)
hbc_rf = ggplot(data = RF_ten_best_features, 
       aes(x = rownames(RF_ten_best_features),
           y = RF_ten_best_features[,1])) +
  geom_col()+ xlab("Feature") + ylab("Importance Score") + coord_flip() +
  ggtitle("10 Most Influential Features in Random Forest")
print(hbc_rf)
# Calculate and print the average AUC and accuracy of 5-round KNN models.
print(paste0("Average AUC for RF: ", mean(RF_AUCs)))
print(paste0("Average accuracy of RF: ", mean(RF_accuracies)))
```

## Support Vector Machine:
``` {r}
# Create vectors containing AUC and accuracy of each round's model.
SVM_AUCs = c()
SVM_accuracies = c()
for(index in 1:5) {
  # Iterate through each train-test sets combination.
  train = get(paste0("ZAS.train", index))
  test = get(paste0("ZAS.test", index))
  # Train the model and get the fitted values.
  SVM = svm(Cath~., data = train)
  test_no_cath = test[, !(colnames(test) %in% c("Cath"))]
  fitted_values = predict(SVM, test_no_cath)
  # Calculate and store the AUC of each round's model.
  pred = prediction(as.numeric(fitted_values), test$Cath)
  AUC = performance(pred, 'auc')@y.values
  SVM_AUCs = append(SVM_AUCs, as.numeric(AUC))
  # Calculate and store the accuracy of each round's model.
  conf_matrix = table(test$Cath, fitted_values, dnn = c("Actual", "Predicted"))
  accuracy = (conf_matrix[1] + conf_matrix[4]) / sum(conf_matrix)
  SVM_accuracies = append(SVM_accuracies, accuracy)
}
# Calculate and print the average AUC and accuracy of 5-round KNN models.
print(paste0("Average AUC for SVM: ", mean(SVM_AUCs)))
print(paste0("Average accuracy of SVM: ", mean(SVM_accuracies)))
```

## Naive Bayes:
``` {r}
bayes = naiveBayes(Cath~., data=ZAS.train1)
Bayes_AUCs = c()
Bayes_accuracies = c()
for(index in 1:5) {
  # Iterate through each train-test sets combination.
  train = get(paste0("ZAS.train", index))
  test = get(paste0("ZAS.test", index))
  # Train the model and get the fitted values.
  r.fit=naiveBayes(Cath~., data=train)
  m.predict = predict(r.fit, test)
  t1 = table(actual = test$Cath, predicted = m.predict)
  # Accuracy
  Accuracy.Tree = (t1[1,1]+t1[2,2])/nrow(test)
  Bayes_accuracies = append(Bayes_accuracies, Accuracy.Tree)

  # AUC
  Rpred.bayes = predict(r.fit, test, type = 'raw') 
  Rpred <- ROCR::prediction( Rpred.bayes[,2], test[["Cath"]]) 
  cauc = performance(Rpred, "auc")
  AUC.Bayes = as.numeric(cauc@y.values)
  Bayes_AUCs = append(Bayes_AUCs, as.numeric(cauc@y.values))

}
# Average
cat(paste0("Average AUC for Naive Bayes: ", round(mean(Bayes_AUCs),digits = 5)))
cat("\n")
cat(paste0("Average accuracy of Naive Bayes: ", round(mean(Bayes_accuracies),digits = 5)))
```

## Decision Tree:
``` {r}
# Create vectors containing AUC and accuracy of each round's model.
Tree_AUCs = c()
Tree_accuracies = c()
for(index in 1:5) {
  # Iterate through each train-test sets combination.
  train = get(paste0("ZAS.train", index))
  test = get(paste0("ZAS.test", index))
  # Train the model and get the fitted values.
  r.fit=tree(Cath~., data=train)
  m.predict = predict(r.fit, test, type = "class")
  t1 = table(actual = test$Cath, predicted = m.predict)
  # Accuracy
  Accuracy.Tree = (t1[1,1]+t1[2,2])/nrow(test)
  Tree_accuracies = append(Tree_accuracies, Accuracy.Tree)
  
  # do predictions as probabilities and draw ROC
  pred.tree = predict(r.fit, test, type = "vector")
  # computing a simple ROC curve (x-axis: fpr, y-axis: tpr)
  # labels are actual values, predictors are probability of class 
  Rpred <- ROCR::prediction(pred.tree[,2], test[["Cath"]]) 
  cauc = performance(Rpred, "auc")
  Tree_AUCs = append(Tree_AUCs, as.numeric(cauc@y.values))

}
# Average
cat(paste0("Average AUC for decsion tree: ", round(mean(Tree_AUCs),digits = 5)))
cat("\n")
cat(paste0("Average accuracy of decision tree: ", round(mean(Tree_accuracies),digits = 5)))
```

## Boosting (Adaptive)

```{r}
#Boosting
# Create vectors containing AUC and accuracy of each round's model.
Boost_AUCs = c()
Boost_accuracies = c()
boost_importance = as.data.frame((matrix(ncol = 0, nrow = 50)))

for(index in 1:5) {
  # Iterate through each train-test sets combination.
  train = get(paste0("ZAS.train", index))
  test = get(paste0("ZAS.test", index))
  # Train the model and get the fitted values.
  boost = boosting(Cath~., data = train)
  boostpredict = predict.boosting(boost, newdata = test)
  #Calculate accuracy of the model
  n = sum(boostpredict$confusion)
  x = diag(boostpredict$confusion)
  accuracy = sum(x)/n
  Boost_accuracies = append(Boost_accuracies, accuracy)
  #Get AUC
  boostprob = prediction(boostpredict$prob[,2], test$Cath)
  AUC = performance(boostprob, "auc")@y.values
  Boost_AUCs = append(Boost_AUCs, as.numeric(AUC))
    # Calculate and store the features importance in each round.
  current_boost_imp = as.data.frame(boost$importance)
  boost_importance = cbind(boost_importance, current_boost_imp)
  names(boost_importance)[length(names(boost_importance))] = paste0("Round_", index)
}
# Calculate, store and show each feature's average importance.
boost_importance = as.data.frame(sort(apply(boost_importance, 1, mean), decreasing = TRUE))
colnames(boost_importance) = "Average Importance"
print(boost_importance)
# Plot the the importance of 10 most influential features in boosting.
boost_ten_best_features = head(boost_importance, 10)
hbc_boost = ggplot(data = boost_ten_best_features, aes(x = rownames(boost_ten_best_features), y = boost_ten_best_features[,1])) +
  geom_col()+ xlab("Feature") + ylab("Importance Score") + coord_flip() +
  ggtitle("10 Most Influential Features in boosting")
print(hbc_boost)
print(paste0("Average AUC for boosting: ", mean(Boost_AUCs)))
print(paste0("Average accuracy of boosting: ", mean(Boost_accuracies)))

```

## Boosting (Gradient):

```{r message=FALSE, warning=FALSE}
#Gradient Boosting
# Create vectors containing AUC and accuracy of each round's model.
gradb_AUCs = c()
gradb_accuracies = c()
for(index in 1:5) {
  # Iterate through each train-test sets combination.
  train = get(paste0("ZAS.train", index))
  test = get(paste0("ZAS.test", index))
  # Train the model and get the fitted values.
  gradb = train(Cath~., data = train, method = "gbm", verbose = FALSE)
  gradbpredict = predict(gradb, test)
  #Calculate accuracy
  conf_matrix = table(test$Cath, gradbpredict, dnn = c("Actual", "Predicted"))
  accuracy = (conf_matrix[1] + conf_matrix[4]) / sum(conf_matrix)
  gradb_accuracies = append(gradb_accuracies, accuracy)
  #Get AUC
  gradbprob = prediction(as.numeric(gradbpredict), test$Cath)
  AUC = performance(gradbprob, "auc")@y.values
  gradb_AUCs = append(gradb_AUCs, as.numeric(AUC))
}
print(paste0("Average AUC for Gradient Boosting: ", mean(gradb_AUCs)))
print(paste0("Average accuracy of Gradient Boosting: ", mean(gradb_accuracies)))

```


## Bagging:

```{r}
#Bagging
# Create vectors containing AUC and accuracy of each round's model.
Bagg_AUCs = c()
Bagg_accuracies = c()
bag_importance = as.data.frame((matrix(ncol = 0, nrow = 50)))
for(index in 1:5) {
  # Iterate through each train-test sets combination.
  train = get(paste0("ZAS.train", index))
  test = get(paste0("ZAS.test", index))
  # Train the model and get the fitted values.
  bagg = bagging(Cath~., data = train)
  baggpredict = predict.bagging(bagg, newdata = test)
  #Calculate accuracy of the model
  n = sum(baggpredict$confusion)
  x = diag(baggpredict$confusion)
  accuracy = sum(x)/n
  Bagg_accuracies = append(Bagg_accuracies, accuracy)
  #Get AUC
  baggprob = prediction(baggpredict$prob[,2], test$Cath)
  AUC = performance(baggprob, "auc")@y.values
  Bagg_AUCs = append(Bagg_AUCs, as.numeric(AUC))
  # Calculate and store the features importance in each round.
  current_bag_imp = as.data.frame(bagg$importance)
  bag_importance = cbind(bag_importance, current_bag_imp)
  names(bag_importance)[length(names(bag_importance))] = paste0("Round_", index)
}
# Calculate, store and show each feature's average importance.
bag_importance = as.data.frame(sort(apply(bag_importance, 1, mean), decreasing = TRUE))
colnames(bag_importance) = "Average Importance"
print(bag_importance)
# Plot the the importance of 10 most influential features in bagging.
bag_ten_best_features = head(bag_importance, 10)
hbc_bag = ggplot(data = bag_ten_best_features, aes(x = rownames(bag_ten_best_features), y = bag_ten_best_features[,1])) +
  geom_col()+ xlab("Feature") + ylab("Importance Score") + coord_flip() +
  ggtitle("10 Most Influential Features in Bagging")
print(hbc_bag)
print(paste0("Average AUC for Bagging: ", mean(Bagg_AUCs)))
print(paste0("Average accuracy of Bagging: ", mean(Bagg_accuracies)))
```


## Section 3: Feature selection from ensemble models.

In this section, 2 compressed datasets will be produced by removing some less important features. This is based on the feature importance information from ensemble models trained in Section 2. Details of feature importance analysis 
will be included in the final report.
``` {r}
# Cut 1 threshold.
cut_1 = c("Q.Wave", "Obesity", "Systolic.Murmur", "LVH", "FH")
ZAS.compressed1 = ZAS[, !(colnames(ZAS) %in% cut_1)]
print(dim(ZAS.compressed1))
# write.csv(ZAS.compressed1, "ZAS.compressed1.csv")

# Cut 2 threshold.
cut_2 = c("Q.Wave", "Obesity", "Systolic.Murmur", "LVH", "FH", "Sex", "DLP", "Function.Class_0", "Function.Class_1", "Function.Class_2", "Function.Class_3", "St.Depression", "DM", "Current.Smoker", "Dyspnea", "VHD_mild", "VHD_Moderate", "VHD_N", "VHD_Severe", "Nonanginal", "HDL", "Neut")
ZAS.compressed2 = ZAS[, !(colnames(ZAS) %in% cut_2)]
print(dim(ZAS.compressed2))
# write.csv(ZAS.compressed2, "ZAS.compressed2.csv")
```
## Section 4: Optimizing and choose the best model.

In this section, the best models from the previous section are selected and further optimised in order to achieve the best performance possible.

In addition, a neural network will be trained and evaluated in this section using the feature-selected data.

## Neural Network:
```{r}
# Train and test sets with threshold 1 applied.
set.seed(316407)
training.rows1 = sample(1:nrow(ZAS.compressed1), 0.7*nrow(ZAS.compressed1))
train1 = ZAS.compressed1[training.rows1,]
test1 = ZAS.compressed1[-training.rows1,]
# Train and test sets with threshold 2 applied.
set.seed(316407)
training.rows2 = sample(1:nrow(ZAS.compressed2), 0.7*nrow(ZAS.compressed2))
train2 = ZAS.compressed2[training.rows2,]
test2 = ZAS.compressed2[-training.rows2,]
# Since the neuralnet() function can only deal with quantitative variables, all 
# qualitative variables need to be converted into dummy variables.
# Threshold 1.
train1_dummy = model.matrix(~., data = train1)[,-1]
test1_dummy = model.matrix(~., data = test1)[,-1]
# Threshold 2.
train2_dummy = model.matrix(~., data = train2)[,-1]
test2_dummy = model.matrix(~., data = test2)[,-1]
# According to thumb of rules from below article, set different numbers of
# hidden layers and neurons.
# https://www.heatonresearch.com/2017/06/01/hidden-layers.html
# Number of hidden layers.
n_hlayers = 3
# Number of neurons for threshold 1.
input_size1 = ncol(train1)
n_hneurons1 = as.integer(c(0.5*input_size1, 2/3*input_size1,
                          input_size1, 2*input_size1))
# Number of neurons for threshold 2.
input_size2 = ncol(train2)
n_hneurons2 = as.integer(c(0.5*input_size2, 2/3*input_size2,
                           input_size2, 2*input_size2))
# For each combination of hidden layers & neurons, train the neural network and 
# evaluate the ROC / AUC.
# Threshold 1.
for (hlayer in 1:n_hlayers) {
  for (hneuron_index in 1:length(n_hneurons1)) {
    # Generate the tuned parameters for this iteration.
    hiddens = rep(n_hneurons1[hneuron_index], times = hlayer)
    # To reduce the randomness of the training, set the random seed.
    set.seed(316407)
    # Train the neural network.
    nnet = neuralnet(Cath1~., train1_dummy, hidden = hiddens, 
                     linear.output = FALSE)
    # Feed the test data into trained model to predict.
    test1_no_cad = test1_dummy[, !(colnames(test1_dummy) %in% c("Cath1"))]
    fitted_values = round(predict(nnet, test1_no_cad))
    pred = ROCR::prediction(as.numeric(fitted_values), test1$Cath)
    # Calculate the AUC and confusion matrix.
    AUC = performance(pred, 'auc')@y.values
    conf_matrix = table(test1$Cath, fitted_values,
                        dnn = c("Actual", "Predicted"))
    # Print the result.
    print("Threshold1: ")
    print(paste0("Number of hidden layers: ",
                 hlayer, 
                 "; Number of neurons in each hidden layer: ", 
                 n_hneurons1[hneuron_index]))
    print(paste0("AUC: ", AUC))
    print(conf_matrix)
  }
}
# Threshold 2.
for (hlayer in 1:n_hlayers) {
  for (hneuron_index in 1:length(n_hneurons2)) {
    # Generate the tuned parameters for this iteration.
    hiddens = rep(n_hneurons2[hneuron_index], times = hlayer)
    # To reduce the randomness of the training, set the random seed.
    set.seed(316407)
    # Train the neural network.
    nnet = neuralnet(Cath1~., train2_dummy, hidden = hiddens, 
                     linear.output = FALSE)
    # Feed the test data into trained model to predict.
    test2_no_cad = test2_dummy[, !(colnames(test2_dummy) %in% c("Cath1"))]
    fitted_values = round(predict(nnet, test2_no_cad))
    pred = ROCR::prediction(as.numeric(fitted_values), test2$Cath)
    # Calculate the AUC and confusion matrix.
    AUC = performance(pred, 'auc')@y.values
    conf_matrix = table(test2$Cath, fitted_values,
                        dnn = c("Actual", "Predicted"))
    # Print the result.
    print("Threshold2: ")
    print(paste0("Number of hidden layers: ",
                 hlayer, 
                 "; Number of neurons in each hidden layer: ", 
                 n_hneurons2[hneuron_index]))
    print(paste0("AUC: ", AUC))
    print(conf_matrix)
  }
}
```

## Bagging and Boosting:

The algorithms were optimised by running multiple iterations with different nbagg values and obtaining the nbagg value with the lowest OOB error.
This chunk of code takes a very long time to run, so is excluded from running in the R Markdown in order to save time when running the whole program.
```{r eval = FALSE}
#Bagging Optimisation
OOB_1 = data.frame("OOB_Error" = c(), "ntrees" = c())
OOB_2 = data.frame("OOB_Error" = c(), "ntrees" = c())

for(index in 1:2) {
  # Iterate through each compressed dataset
  ZAS_Data = get(paste0("ZAS.compressed", index))
  
  # Set the seed
  set.seed(3164)
  # Get 70% of the dataset to be training dataset.
  training.rows = sample(1:nrow(ZAS_Data), 0.7*nrow(ZAS_Data))
  ZAS.train = ZAS_Data[training.rows,]
  # The rest of rows are testing dataset.
  ZAS.test = ZAS_Data[-training.rows,]
  
  for (ntrees in 100:200) {
    bagg = train(Cath~., data = ZAS.train, method = "treebag", nbagg = ntrees)
    newrow = c(bagg$results$Accuracy, ntrees)
    if (index == 1) {
      OOB_1 = rbind(OOB_1, newrow)
    }
    else {
      OOB_2 = rbind(OOB_2, newrow)
    }
  }
  ggplot(data = get(paste0("OOB_", index)), aes(x = ntrees, y = OOB_error)) + geom_line()
}


#Boosting Optimisation
OOB_1 = data.frame("OOB_Error" = c(), "ntrees" = c())
OOB_2 = data.frame("OOB_Error" = c(), "ntrees" = c())

for(index in 1:2) {
  # Iterate through each compressed dataset
  ZAS_Data = get(paste0("ZAS.compressed", index))
  
  # Set the seed
  set.seed(3164)
  # Get 70% of the dataset to be training dataset.
  training.rows = sample(1:nrow(ZAS_Data), 0.7*nrow(ZAS_Data))
  ZAS.train = ZAS_Data[training.rows,]
  # The rest of rows are testing dataset.
  ZAS.test = ZAS_Data[-training.rows,]
  
  for (ntrees in 25:50) {
    boost = train(Cath~., data = ZAS.train, method = "adaboost", nbagg = ntrees)
    newrow = c(boost$results$Accuracy, ntrees)
    if (index == 1) {
      OOB_1 = rbind(OOB_1, newrow)
    }
    else {
      OOB_2 = rbind(OOB_2, newrow)
    }
  }
  ggplot(data = get(paste0("OOB_", index)), aes(x = ntrees, y = OOB_error)) + geom_line()
}
```

## Section 4: Creating the Final Model

The final model chosen is an adaptive boosting model.
The code below creates the model and also evaluates its performance.
```{r}
# Model seems to output different results if the training/testing dataset creation is excluded from the code. I have left it in here as it preserves the original performance of the model.

# Create training and testing datasets
set.seed(3164)
# Get 70% of the dataset to be training dataset.
training.rows = sample(1:nrow(ZAS.compressed1), 0.7*nrow(ZAS.compressed1))
ZAS.train1 = ZAS.compressed1[training.rows,]
# The rest of rows are testing dataset.
ZAS.test1 = ZAS.compressed1[-training.rows,]

training.rows = sample(1:nrow(ZAS.compressed2), 0.7*nrow(ZAS.compressed2))
ZAS.train2 = ZAS.compressed2[training.rows,]
# The rest of rows are testing dataset.
ZAS.test2 = ZAS.compressed2[-training.rows,]

#Train the optimised boosting model
boost = adabag::boosting(Cath~., data = ZAS.train2, mfinal = 40)
# # Save the best model as an individual data file so it can be re-loaded and 
# # used later for the final webpage.
# save(boost, file = "boost_model.Rdata")
boostpredict = predict.boosting(boost, newdata = ZAS.test2)
#Calculate accuracy of the model
n = sum(boostpredict$confusion)
x = diag(boostpredict$confusion)
accuracy = sum(x)/n
boostprob = ROCR::prediction(boostpredict$prob[,2], ZAS.test2$Cath)
boost_AUC = performance(boostprob, "auc")@y.values
print(paste0("Model AUC: ",as.numeric(boost_AUC)))
print(paste0("Model Accuracy: ", accuracy))
```

